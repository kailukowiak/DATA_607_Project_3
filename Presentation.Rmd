---
title: "Project 3 Presentation"
author: "Dream Team"
date: '2017-10-21'
output:
  ioslides_presentation:
    widescreen: true
    smaller: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r message= FALSE, warning=FALSE}
library(RCurl)
library(xml2)
library(rvest)
library(knitr)
library(tidyr)
library(dplyr)
library(readr)
library(mongolite)
library(ggplot2)
library(ggthemes)
library(dummies)
library(wordcloud2)
```

```{r}
m <- mongo(url = "mongodb://127.0.0.1:27017" ) # Database ports
mt <- mongo("skillsDF") # Data base name
if(mt$count() > 0) mt$drop() # Dropping the database so we don't duplicate data.
mt$import(file("skillsDF.json"))
df <- mt$find() # Create DF
```

```{r}
# This code seperates the skills based on each job.
skillCount <- paste(df$ExperienceList, collapse = ",") 
skillCount <-  strsplit(skillCount, ",")[[1]] # Splits the stings on , 
skillCount <- gsub(" ", "", skillCount) # Removes all white space
skillTable <-  table(skillCount) %>% as.data.frame()
```

## 

```{r}
wordTable <- skillTable
wordTable$Freq <- sqrt(wordTable$Freq) # Square root so that lesser known skills still show up. 
set.seed(212)
wordcloud2(wordTable, size = .2) # Size must be smaller to avoid ML not showing up.

```



## Cities

We scraped indeed.com for jobs with the title data scientist in:

* New York
* Boston
* San Francisco
* Houston 
* Seattle. 

## Data
```{r}
head(df) %>% kable() # The head of the original database
```


## Most valued skills

```{r fig.height=5, fig.width=10}
ggplot(skillTable, aes(x = reorder(skillCount, -Freq), y = Freq)) + # Takes data from skill count. 
  geom_bar(stat = "identity") + # Simply sum with bar geom.
  ggtitle("Required skills") + # Titles
  xlab("Skill") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) # Rotated 90 to make it legible. 
```


```{r}
# Create KNN matrix
# This is legacy code from a misguided attempt to have do a KNN un-supervised learning.
# It proved useful for other analysis. 
df$ExperienceList <- as.character(df$ExperienceList)
df$ExperienceList <- gsub("\\s","", df$ExperienceList) # Removes white space
dfKNN <- df %>%
  mutate(ID = 1:n()) %>%
  mutate(ExperienceList = strsplit(ExperienceList, split = ",")) %>%
  unnest() %>%
  mutate(Value = 1) %>%
  spread(ExperienceList, Value, fill = 0) 
```


```{r}
dfCity <- dfKNN %>% # 
  select(-c(CompanyName, TitleName, ID)) %>% # Drops uncessary columns
  group_by(City) %>% 
  filter(n() > 4) %>% # Removes cities that might have very few jobs that were biasing the sample. 
  summarise_all(funs(mean(., na.rm = TRUE)))

dfCityClean <- dfCity %>% mutate(sumVar = rowSums(.[2:ncol(dfCity)])) %>% 
  select(City, sumVar) %>% 
  arrange(-sumVar) # Cleanes the dataset so it can be compared to just the jobs.

```


## Diversity of Skills
```{r}
ggplot(dfCityClean, aes(x = reorder(City, -sumVar), y = sumVar)) + 
  geom_bar(stat = "identity") +
  ggtitle("Diversity of Skills") +
  xlab("City") +
  ylab("Diversity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Best City for Major Skills

```{r}
# Calculates the city with the highest requirements for top non-generic skills
vec <- c( "Python", "R", "Hadoop", "Spark", "Java")
Python <- as.character(dfCity[which.max(dfCity$Python), 1])
R <- as.character(dfCity[which.max(dfCity$R), 1])
Hadoop <- as.character(dfCity[which.max(dfCity$Hadoop), 1]) # As character necessary to avoid anything screwy. 
Spark <- as.character(dfCity[which.max(dfCity$Spark), 1])
Java <- as.character(dfCity[which.max(dfCity$Java), 1])

BestCity <- as.data.frame(c(Python, R, Hadoop, Spark, Java), row.names = vec)

colnames(BestCity) <- "City"
BestCity %>% kable()
```


## Areas with the most work

```{r}
df %>% 
  group_by(City) %>% 
  count() %>% 
  arrange(-n) %>% 
  ggplot( aes(x = reorder(City, -n), y = n)) + 
  geom_bar(stat = "identity") +
  ggtitle("Number of Jobs") +
  xlab("City") +
  ylab("Number of Jobs")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


## Names and Roles

* Kai Lukowiak
    + Project Cordinator
* James Kuruvilla
    + Graphics and Database
* Michael D'Acampora
    + Graphics and CSS
* Gheorghe Becciv
    + Planning and Scraping
